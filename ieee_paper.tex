\documentclass[10pt,conference,compsocconf]{IEEEtran}
\input{support/macros} % color is defined in macros or misc_mac
\input{support/misc_mac}
\input{support/setupicase}


\def\red#1{\textbf{\textcolor{red}{#1}}}
\def\blue#1{\textbf{\textcolor{blue}{#1}}}
\def\qes#1{{\blue{*** For Erik: #1 ***}}}
\def\es#1{{\blue{*** For Erik: #1 ***}}}
\def\ge#1{{\red{*** For Gordon: #1 ***}}}
\def\ttt#1{{\tt #1}}
\def\bold#1{{\bf #1}}

\usepackage{soul}
\usepackage{xspace}
\usepackage{color} 
\definecolor{darkgreen}{rgb}{0,0.5,0}
\usepackage[colorlinks=true,% 
  linkcolor=red,% 
  citecolor=darkgreen,%
  urlcolor=blue]{hyperref}

\usepackage{listings}


\newcommand{\todo}[1]{{\color{red}\textbf{\hl{#1}}\xspace}}

\def\qes#1{}
\def\es#1{}
%\def\ge#1{}
%\usepackage{morefloats}

\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}

\usepackage{amsmath}
\usepackage{amssymb}


\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
%\usepackage{fixltx2e}
\usepackage{url}
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Sparse Matrix Vector Multiplication with Multiple vectors and
  Multiple Matrices on the MIC Architecture}


\author{\IEEEauthorblockN{Gordon Erlebacher\IEEEauthorrefmark{1},
Erik Saule\IEEEauthorrefmark{2}, Natasha Flyer\IEEEauthorrefmark{3}, 
and Evan Bollig\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Department of Scientific Computing, 
Florida State University, Tallahassee, FL 32306-4120\\
Email: gerlebacher@fsu.edu}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Department of Computer Science, University of North Carolina at Charlotte\\
Email: esaule@uncc.edu}
\IEEEauthorblockA{\IEEEauthorrefmark{3}Computational and Information Systems Laboratory, UCAR \\
Email: flyer@ucar.edu}
\IEEEauthorblockA{\IEEEauthorrefmark{4}Department of Scientific Computing, Florida State University\\
Email: bollig@gmail.com}}
\maketitle


\begin{abstract}
In this paper, we develop an efficient scheme for the calculation of
derivatives within the context of Radial Basis Function
Finite-Difference (RBFFD). RBF methods express functions as a linear
combination of radial basis functions on an arbitrary set of
nodes. The Finite-Difference component expresses this combination over
a local set of nodes neighboring the point where the derivative is
sought.  The derivative at all points takes the form of a sparse
matrix/vector multiplication (spmv).

In this paper, we consider the case of local stencils with the number
of nodes at each point and encode the sparse matrix in ELLPACK
format. We increase the number of operations relative to memory
bandwidth by calculating four derivatives of four different functions,
or 16 different derivatives. We demonstrate a novel implementation on
the MIC architecture, taking into account its advanced swizzling and
channel interchange features. We present benchmarks that show an
almost order of magnitude increase in speed compared to efficient
implementations of a single derivative. We explain the results through
consideration of operation count versus memory bandwidth.
\end{abstract}

\begin{IEEEkeywords}
OpenMP; MIC; spmv; sparse matrix; Radial Basis Function;
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
The multiplication of sparse matrix by a dense vector (SpMV) is an
important kernel in many applied fields such as fluid dynamics 
\cite{journals/tog/BolzFGS03} 
to graph analysis (recommender systems~\cite{Brin98}, graph
drawing~\cite{Koren05}). So improving the performance of SpMV has
captured the interest of many researcher; including the development of
various implementations for multi-core
CPUs~\cite{Buluc2009SPAA,Williams07} and GPUs~\cite{Bell08,
  conf/ipps/KreutzerHWFBB12,
  journals/concurrency/VazquezFG11,kumar2012accelerating}. The main
challenge that is faced to obtain good performance for SpMV is that
the operation are conducted using memory location that are irregular
and often unpredictible. That makes the kernels mostly memory
bound and there is a significant instruction overhead per flop.

Common improvement techniques such as register blocking, bandwidth
reduction (matrix reordering~\cite{Cuthill69}), partitioning to fit in
cache or TLB~\cite{Nishtala07,Temam:1992:CBS:147877.148091,conf/ppsc/Toledo97},
unrolling~\cite{Mellor-Crummey04} have impacts which are very
dependent on the matrix and overall do not lead to dramatic
improvement. (The state-of-the-art techniques in OSKI~\cite{Vuduc05}
provide some useful yet limited improvements). Register blocking~\cite{conf/ppsc/Toledo97} does
not apply well to most the matrix at hand (despite it can be applied
with virtually no overhead in all cases thanks to compressed
representation~\cite{Buluc11}). There is about 8 bytes of the matrix to move
in per non zero (assuming single precision); each nonzero requires two
floating point operations leading to a flop-to-byte ratio of at most
$\frac{1}{4}$. This limits the obtained performance to at most a
quarter of the bandwidth of the architecture wasting a lot of
potentially useful cycles. The commonly used techniques are mostly
designed to reach that bound rather than overcome it.

Fortunately that fate is not inevitable. One solution would be to
schedule a more instruction intensive kernel simultaneously with the
execution of SpMV, relying on some hardware threading capabilities,
such as HyperThreading, to reduce the cycle wastage. However, most ot
the applications that use SpMV do not typically have an instruction
intensive kernel to run simultaneously.

An other solution, and the one we pursue in this paper, is to compute
multiple SpMV at once on matrices that have the same sparsity
patterns. Obviously not all the applications have such a property. But
important classes of applications such as graph
recommendation~\cite{Kucuktunc13-SNAM}, eigensolving~\cite{LOBPCG} and the computation of
derivatives of Radial Basis Functions(RBF)~\cite{FLBWSC12} can use multiple
SpMVs simultaneously. In this paper in particular, we investigate the
case of the derivative of RBFs where four derivatives of four
different functions is expressed as the multiplication of four vectors
by four matrices with identical sparsity patterns leading to the
simultaneous execution of 16 SpMVs at a time. Using multiple vectors
at a time has been investigated before to densify the
computations~\cite{Im01} but this is the first time that the
densification comes from adding both vectors and matrices.

To perform our analysis, we focus our attention on the improvement
that we can achieve on the Intel Xeon Phi processor. It follow the
Many Integrated Core (MIC) architecture, which has a significant
memory bandwidth and peak flop throughput thanks to its 512-bit large
SIMD registers. The Xeon Phi processor has been shown to be promising
for sparse linear algebra compared to more classical CPU or GPU
architecture~\cite{Saule13-ARXIV, Liu:2013:ESM:2464996.2465013, cramer2012openmp}.

In Section~\ref{sec:rbf} we present the computation of RBFs and how it
can be expressed 16 multiplication of 4 vectors by 4 sparse matrices
with a common sparsity pattern. Section~\ref{sec:model} presents an
estimation of the instruction intensity of various form of the
computations and we show that a 6- fold improvement can be
expected when computing the 16 multiplications simultaneously and
reach a total of about 210 Gflop/s. This performance represents
approximatively 10\% of the available flop/s of a Knight Corner
coprocessor. Therefore, it is necessary to have implementations that
perform the computation in as little amount of instructions as
possible. We describe in Section~\ref{sec:impl} the details of the MIC
architecture and how to use specialized load, store, swizzle and
permutation instruction to efficiently bring the data in the vector
registers to be processed. Section~\ref{sec:expe} gives some
experimental result about the amount of bandwidth that can be achieved
depending on how the SpMV kernel is written and the actual performance
of the various kernel on multiple classes of matrices some generated
for analysis purpose and some extracted from and RBF application. A
performance of 135 Gflop/s is achieved on real scenario. Concluding
remarks and perspectives are provided in Section~\ref{sec:ccl}.

\section{Derivatives of Radial Basis Functions}
\label{sec:rbf}

%In the theory of Radial Basis Functions Finite-Difference (RBFFD),
%derivatives of a function $f(\rvec)$ at node $i$ are expressed as a
%linear combination of the function values at the stencil center and
%the nodes connected to node $i$ (Figure~\ref{fig:rbf_stencils}). Thus
%$y$ is a discrete derivative of the vector $x$.

In this paper, we propose a new idea that is applicable to radial
basis functions (RBFs). Their strength is the ability to randomly
distribute points across complex physical domains, and have an
implementation that is independent of dimensionality. RBFs approximate
a function $f(\xvec)\subset \mathbb{R}^d$ sampled at a set of $N$
distinct point locations, $x_j$, by linearly combining translates of a
single radially symmetric function $\phi(r)$, where $r =
\|\xvec-\xvec_{j}\|$ denotes the Euclidean distance (e.g., in 2-D
$\sqrt{(x-x_j)^2+(y-y_j)^2}$) between where the function is evaluated
$\xvec$ and where the RBF is centered $\xvec_{j}$. That is, the
interpolant is $s(\xvec) = \sum_{j=1}^{N} w_j
\phi_i(\|\xvec-\xvec_{j}\|)$. The weights $w_j$ are obtained by
inverting the system
\begin{equation}
\parray{lccr}{
\phi_{11} & \phi_{12} & \cdots & \phi_{1N} \\
\vdots & \ddots & \vdots & \vdots \\
\phi_{n1} & \phi_{n2} & \cdots & \phi_{NN} 
}
\parray{c}{ w_{1} \\ \vdots \\ w_{N} }
=
\parray{c}{ f(\xvec_1) \\ \vdots\\ f(\xvec_N) }. 
\label{eq:rbf}
\end{equation}
where $\phi_{ij} = \phi(\|\xvec_i-\xvec_j\|)$. 
%\end{equation}
%\begin{equation}
%\begin{bmatrix}
%\phi(\|\xvec_{1}-\xvec_{1}\|) & \phi(\|\xvec_{1}-\xvec_{2}\|) & \cdots & \phi(\|\xvec_{1}-\xvec_{N}\|) \\
%\vdots & \ddots & \vdots  \\
%\phi (\|\xvec_{n}-\xvec_{1}\|) & \phi(\|\xvec_{n}-\xvec_{2}\|) & \cdots & \phi(\|\xvec_{N}-\xvec_{N}\|)
%\end{bmatrix}

The RBF differentiation matrix, $D_N$, is derived by applying the
desired analytic derivative operator $L$ to the RBF interpolant
$s(\xvec)$ above and evaluating it at the point locations. For very
large problems, this is a computationally expensive since the matrix
in (\ref{eq:rbf}) is full and inversion requires O$(N^3)$
operations. To alleviate the cost of this global approach (i.e. using
every node in the domain to calculate the derivative at a given node
$\xvec_i$), RBF-generated finite differences (RBF-FD) have been
derived \cite{TAI1,TAI2,SDY02,WrFo06,FoL11,FLBWSC12}. RBF-FD uses only
a local set of the $n_z-1$ nearest neighbors to the point $\xvec_i$ to
approximate the derivative. In other words,
$Lf(\xvec_i)=\sum_{j=1}^{n_z}a_jf(\xvec_j)$. The differentiation
weights, $a_j$, are calculated by enforcing that this linear
combination should be exact for RBFs,
$\{\phi(\|\xvec-\xvec_{j}\|)\}_{j=1}^{n_z}$, centered at each of the
node locations $\{\xvec_j\}_{j=1}^{n_z}$ (classical finite differences
(FD) would enforce that it be exact for polynomials instead). Similar
to FD, as the stencil size $n_z$ increases so does the order of the
method.

For a total of $N$ points, there will be $N$ linear systems to solve,
each of size $n_z \times n_z$. Each linear solve produces a row of the
RBF-FD differentiation matrix $D_{n_z}$, resulting in a $N \times N$
matrix with $n_zN$ nonzero entries. To evaluate the derivative at all
points in the domain, takes the form
$$
  \mathbf{g} = D_{n_z} \mathbf{f}
$$

where $f$ is the source vector of function values and $g$ is the
resulting vector of derivative values. The computation of a single
derivative has been reduced to a SpMV, where each row has $n_z$
nonzeros. In
practice, $n_z=32$ in two-dimensional flows and 64 or 100 for
three-dimensional flows. These numbers are similar to what is used in
finite-element codes. Notice that the non zero elements of the matrix
correspond to a relation of nearest neighbor. This relationship is not
necessarily symmetric as shown in Figure~\ref{fig:rbf_stencils} which
means that the matrix sparsity pattern might not be symmetric either.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=\linewidth]{figures/rbf_stencils.pdf}
  \caption{RBFFD Stencils. Each node of the stencil is connected to
    $n_z-1$ stencil nodes in addition to itself. In the figure, node
    $A$ is connected to $B$, but $B$ is {\em not\/} connected to
    $A$. Thus adjacency graph of $A$ is not-symmetric.}
  \label{fig:rbf_stencils}
\end{figure}

Many problems in fluid dynamics and in the geosciences require the
solution to transport equations of the form
$$
\pf{Q}{t} = f(Q,Q_x, Q_y, Q_z, \Laplacian{Q})
$$
where $Q$ is a vector of unknowns (3 components of velocity, pressure,
temperature). For example, in solving a system of equations, it is
often necessary to compute the $x$ derivative of multiple functions,
typically four for the Euler equations, or five for the Navier-Stokes
equations. Multiple right-hand sides transform a SpMV into a SpMM
(Sparse Matrix/dense Matrix multiplication), which improves register
utilization and decreases cache misses by vectorizing over the
multiple source vectors. Further improvements are possible by
recognizing that different derivative matrices (x,y,z and Laplacian
for example), have the same sparsity distribution; only the weight
change.  Thus, alternative to computing a derivative of multiple
functions, we can calculate multiple derivatives of a single
functions. The increased memory bandwidth due to an increase in the
number of derivative matrices is offset by better cache utilization,
leading to an overall benefit.

% GORDON I WOULD ELIMINATE BELOW AND END HERE.

%**** PUT ELSEWHERE OR ELIMINATE? ****
%
%When solving a one dimensional system of PDEs, one might require an
%$x$ derivative of multiple functions. For example, the Euler equations
%require the $x$ derivative of the three components of velocity and
%pressure. In this case, there are $n_v=4$ vectors $x^k$,
%$k=0,\cdots,3$.
%
%Thus for each vector element $y_i$, we compute $y_i = \sum_j A_{ij}
%x_j$. If $A_i$ is row $i$ of $A$, $y_i$ is simply the dot product $A_i
%x$. The next level of generality is to consider $n_v$ vectors $x^k$,
%$k=0,\cdots,n_v-1$. Whatever the spmv implementation, one achieves
%improved performance if the matrix formed from the columns $x^k$ are
%stored in row major order. Thus, $x^0_0,x^1_0,\cdots,x^{n_v-1}_0$, are
%stored in consecutive memory location. The random access of the
%elements of $x$ is thus reduced. Maximum efficiency is achieved when
%$n_v=16$ floats or $8$ doubles, given that cache lines take 64
%bytes. We will benchmark this case, labeled $Svn$, where $n$ refers to
%the number of vectors (Iv4 uses four vectors). The $S$ refers to singe
%precision. A double precision run is labelled $Dvn$.
%
%Alternatively, when solving a PDE, one might require derivatives of a
%given scalar function with respect to coordinate directions $x$, $y$,
%$z$. Second order operators of often required, such as a second
%derivative with respect to $x$ or a Laplacian operator. In the RBFFD
%formulation, on can compute different derivatives using the same
%stencil, but with different weights. In other words, the adjacency
%matrix that corresponds to $A$ remains constant, but the matrix
%elements of $A$ change with the particular derivative.  In this case,
%label with a superscript $l$ the particular matrix $A^k$. Since the
%adjacency matrix is assumed invariant, there is only need for a single
%matrix \ttt{C{ij}}. In Ellpack format, each row is of constant size
%(the number of nonzeros per row of $A$. $C_{ij}$ is the column number
%that locates the $j^{th}$ nonzero in row $i$ of $A$.
%
%
%\todo{Must rewrite the above to make it more focused on the
% application. Remove the architectural/implementation details such as
%  using ELLPACK. Let's focus on on why this computation is important
%  and how does the 16 multiplication appears in the equations.}


In term of memory layout, we choose a format similar to the ELLPACK
format to store the matrix since the number of non zero per row is
constant throughout the matrix. The matrix is given in two arrays. The
first one is the classical {\tt col\_id} array that describes the
sparsity structure of the matrix. For each row it consists of $n_z$
consecutive entries which give in which column is the non zero. The
other array is the {\tt data} array which gives the values of the non
zero in the four matrices. The values of the matrices are interleaved
so that the value $A^l_{col\_id[k]}$ of the $k$th non zero of the
$l$th matrix is at index $(k-1)*4+l-1$. The vectors {\tt x} are also
similarly interleaved so that {\tt x}$^l_i$ and {\tt x}$^{l+1}_i$ are
consecutive in memory. The vector {\tt y} is similarly
interleaved. This is schematically shown in Figure~\ref{fig:mat_struct}.


\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/matrix_not.pdf}
  \caption{Matrix Structure.}
  \label{fig:mat_struct}
\end{figure}



\section{Modelization of the Potential Improvements}
\label{sec:model}

We saw in the previous section that one can express the RBF problem as
a multiplication of four matrices by four vectors. We present here an
estimation of the variation on the flop intensity of the computation
and its impact on the expected performance of the
application. Relevant notations are given in Table~\ref{tab:not}.

\begin{figure}[bth]
  \begin{center}
    \scalebox{.9}{
      \begin{tabular}{|c|l|}
        %\hline
        %& & \\
        \hline
        $b_i$ & number of bytes per index \\
        $b_x$ & number of bytes per value \\
        $n_z$ & number of nonzeros per row $A$ \\
        $n_r$ & number of column/rows of $A$ \\
        $n_c$ & total number of non-zero\\
        $n_v$ & number of {\tt x} vectors \\
        $n_m$ & number of matrices \\
        $s_M$ & size of the matrice(s) in bytes\\
        $s_x$ & size of the {\tt x} vector(s) in bytes\\
        $s_y$ & size of the {\tt y} vector(s) in bytes\\
        \hline
        $cl$    & size of a cache line in bytes\\
        $b_{wT}$ & number of bytes written to memory  \\
        $b_{rT}$ & minimum number of bytes read from memory  \\
        $b_T$   & minimum number of bytes transferred  \\
        $B_{rT}$ & maximum number of bytes read from memory \\
        $B_T$   & maximum number of bytes transferred  \\
        $O$     & number of floating point operations \\
        $I_b$   & maximal computational intensity\\
        $I_w$   & minimum computational intensity\\        
        \hline
      \end{tabular}
    }
  \end{center}
  \caption{Notations}
  \label{tab:not}
\end{figure}

Each vector in the problem is of dimension $n_r$ and each entry
takes $b_x$ bytes. There are $n_v$ {\tt x} vectors and $n_v n_m$ {\tt
  y} vectors, which lead to the size of the {\tt x} and {\tt y} vectors:
$$s_x = n_v b_x n_r$$ $$s_y = n_v n_m b_x n_r$$

The matrix is composed of $n_r$ rows and columns with $n_z$ non-zeros
per row leading to a total of $$n_c = n_r n_z$$ non-zero entries in
the matrix. Each of these non-zero entries has one index of size $b_i$
and $n_m$ values of size $b_x$. The matrices have a total size
of $$s_M = n_c (b_i + b_x n_m) = n_r n_z (b_i + b_x n_m)$$

If we assume an algorithm where the rows are processed the one after
the other, the amount of memory written is precisely the
size of the {\tt y} vector. (This assumption removes the possibility of
blocking or cache partitioning techniques.) $$b_{wT} = s_y = n_v n_m b_x n_r$$

The amount of data read from memory depends highly on both the
algorithm's execution path, and on how the matrix is structured. But
in the best case both the matrix $A$ and the source vector {\tt x} are
read once from the main memory. (We assume that all the elements of
$\tt x$ are involved in the SpMV.)  Thus, $$b_{rT} = s_M + s_x = n_r
n_z (b_i + b_x n_m) + n_v b_x n_r$$
 $$b_T = b_{rT} + b_{wT} =  n_r n_z (b_i + b_x n_m) + n_v b_x n_r (1 + n_m)$$

Notice that there is no reason for a piece of the matrix to be read
multiple times. But assuming that each element of the {\tt x} vector
is read a single time is a strong assumption. If using a single core,
it assumes that either the cache of the architecture can store the full
{\tt x} vector or that the matrix is sufficiently well structured 
to cause no cache trashing. If using multiple cores, this assumes that
no element of the {\tt x} vectors will be used by multiple
cores. \cite{Saule13-ARXIV} showed that there is very little cache trashing
in practice, but it showed that having elements of the vectors used by
multiple cores can have a significant impact on the performance
(growing with $n_v$).

On the other hand, in the worst case, every time the {\tt x} vector is
accessed, the value needs to be transfered from memory again. So in
total, there are as many transfers as the number of non-zeros in the
matrix. Note however that most architectures cannot read memory a
single byte at a time. Instead, a minimum number of bytes, equal to
the size of a cacheline $cl$, are transfered at once.  When there are
multiple vectors, each non-zero element uses $n_v$ consecutive
entries. The worst case number of bytes read and transfered are
$$B_{rT} = s_M + n_c cl \ceil{\frac{n_vb_x}{cl}} $$ 
$$B_T = n_v n_m b_x n_r + n_r n_z \left ( b_i + b_x n_m +  cl \ceil{\frac{n_vb_x}{cl}} \right)$$

In SpMV, each non-zero of the matrix requires two floating point
operations: one for performing the multiplication and one for
accumulating the result row-wise. Here we are dealing with $n_v n_m$
simultaneous SpMVs and the number of floating point operations is
$$O = 2 n_v n_m n_c = 2 n_v n_m n_z n_r$$

The computation intensity is the amount of computations performed per
byte transfered. In the worst case and in the best case, we have
$$I_b = \frac{O}{b_T} = \frac{2 n_v n_m}{ (b_i + b_x n_m) + n_v n_m b_x n_z^{-1} + n_v b_x n_z^{-1} }$$
%$$I_w = \frac{O}{B_T} = \frac{2 n_v n_m}{n_v n_m b_x n_z^{-1} + \left ( b_i + b_x n_m +  cl \ceil{\frac{n_vb_x}{cl}} \right)}$$
$$I_w = \frac{O}{B_T} = \frac{2 n_v n_m}{(b_i+b_x n_m) + n_v n_m b_x n_z^{-1} + cl \ceil{\frac{n_vb_x}{cl}} }$$

%% In the special case $n_v=n_m=1$ in single precision
%% $$I_b = \frac{1}{4} \frac{ n_z }{ n_z + 1} \approx \frac{1}{4}$$
%% %$$I_w = \frac{n_z }{2  + 36  n_z} \approx \frac{1}{36}$$
%% $$I_w = \frac{n_z }{2  + (4+cl/2)  n_z} \approx \frac{1}{4+cl/2}$$

%% In the special case $n_v=n_m=4$ in single precision
%% $$I_b = \frac{32 n_z }{ n_z 20 + 80} \approx \frac{32}{20} = \frac{8}{5} $$
%% %$$I_w = \frac{32 n_z }{64 + n_z 84} \approx \frac{32}{84} = \frac{8}{21} $$
%% $$I_w = \frac{32 n_z }{64 + (20+cl) n_z} \approx \frac{32}{84} = \frac{8}{21} $$
%% The above formulas have been applied to MIC, which has a cache size $cl=64$ bytes.

%% \begin{table}
%%   \centering
%%   \begin{tabular}{|l|r|r|r|r|}
%%     \hline
%%                            & v1m1  & v1m4  & v4m1   & v4m4  \\
%%     \hline
%%     Best Single Precision  & 36.36 & 58.18 & 133.33 & 213.33\\
%%     Worst Single Precision &  4.15 & 14.20 &  16.55 &  55.81\\
%%     Best Double Precision  & 24.00 & 32.21 &  85.71 & 117.07\\
%%     Worst Double Precision &  3.93 & 11.88 &  15.58 &  46.15\\
%%     \hline
%%   \end{tabular}
%%   \caption{Estimation of performance in GFlop/s at 150GB/s for a
%%     matrix of $n_z = 32$}
%% \end{table}

Figure~\ref{fig:ratio_bytes_flops} presents the flop to byte ratios
for single precision computation in the best and the worst case on a
classical cache based architecture ($cl = 64$) and assuming $cl=1$. We
can easily see the potential improvement in the computational
intensity when the number of matrices of vectors increase. There is a
significant difference between the best and the worst case: there is a
$8$ fold difference in the 1 matrix 1 vector case but that gap closes
with the increase in the number of vectors and matrices to a 4 fold
difference in the 4 matrices and 4 vectors case and 1.6 fold in the 16
matrices and 16 vectors. The ratios computed with $cl = 1$ are mostly
similar to the the one with $cl=64$ when the number of vectors is
large. But the differences are important when the number of vector is
low: this highlight that accessing the memory per batch is the main
problem faced when performing a standard SpMV computation.

\begin{figure*}
  \centering
  \subfigure[Best case.]{\includegraphics[width=.33\linewidth]{figures/flops_to_bytes_best-crop.pdf}\label{fig:ratio_best}}%
  \subfigure[Worst case.]{\includegraphics[width=.33\linewidth]{figures/flops_to_bytes_worst-crop.pdf}\label{fig:ratio_worst}}%
  \subfigure[Worst case (No cacheline effects)]{\includegraphics[width=.33\linewidth]{figures/flops_to_bytes_no_cache-crop.pdf}\label{fig:ratio_worst_nocache}}
  
  \caption{Ratio of flops to bytes in single precision: in the best case in
    \ref{fig:ratio_best} best; in the worst case when $cl=64$ in
    \ref{fig:ratio_worst}; and in the worst case neglecting that the
    memory transfers are by batch (equivalent to $cl = 1$) in
    \ref{fig:ratio_worst_nocache}.}
  \label{fig:ratio_bytes_flops}
\end{figure*}

Figure~\ref{fig:ratio_bytes_flops} shows how these ratios translate
into actual performance. This figure provides projected best and worst
Gflop/s achievable assuming the computation is memory bound and the
architecture reaches 150Gflop/s. Notice that \cite{Saule13-ARXIV}
showed a higher peak bandwidth, but we will show in
Section~\ref{sec:expe} why 150Gflop/s is a better estimate of what one
might achieve in these kernels. In single precision, the worse that
one could achieve by using 4 vectors and 4 matrices is much higher
than the best achievable using a classical SpMV computation. The best
performance reachable is 210Gflop/s: almost 6 times higher than the
peak of the classical SpMV case. 


\begin{figure*}
  \centering 
  \subfigure[Absolute performance ]{\includegraphics[width=.49\linewidth]{figures/gflops_peak.pdf}\label{fig:gflops_peak_perf}}
%
  \subfigure[Relative performance to using one matrix and one
    vector.]{\includegraphics[width=.49\linewidth]{figures/speedup_wrt_base.pdf}\label{fig:speedup}}
  
  \caption{Estimation of the reachable performance one can achieve
    using a device with a memory to computational units of 150GB/s
    when varying the number of vectors and matrices. \todo{we could
      remove the speedup plot if we need space.}}
  \label{fig:perf_predict}
\end{figure*}

\section{Efficient Implementation on the Intel Xeon Phi processor}
\label{sec:impl}

\subsection{Knights Corner}

In this work, we use an Intel Xeon Phi 5110P coprocessor. This card
has 8 memory controllers where each of them can execute 5 billion
transactions per second and has two 32-bit channels, achieving a total
bandwidth of 320GB/s aggregated across all the memory
controllers. There are 60 cores clocked at 1.05GHz. Their memory
interfaces are 32-bit wide with two channels and the total bandwidth
is 8.4GB/s per core. Thus, the cores should be able to consume 504GB/s
at most. However, the bandwidth between the cores and the memory
controllers is limited by the ring network that connect the cores and
the memory controller. Its precise bandwidth is unknown but it is
believed to be between 200GB/s and 250GB/s.

Each core in the architecture has a 32kB L1 data cache, a 32kB L1
instruction cache, and a 512kB L2 cache. The architecture of a core is
based the traditional Pentium architecture which have been extended to
64-bit. A core can hold 4 hardware contexts at any time. And at each
clock cycle, instructions from a single thread are executed. Due to
some hardware constraints, two hardware contexts must be used to reach
the peak instruction throughput in that architecture. As in the
Pentium architecture, a core has two different concurrent instruction
pipelines which allow the execution of two instructions per
cycle. However, only one vector or floating point instruction can be
executed at each cycle.

Most of the performance of the architecture comes from the vector
processing unit. Each core has 32 512-bit SIMD registers which can be
used for double or single precision, that is, either as a vector of 8
64-bit values or as a vector of 16 32-bit values, respectively. The
vector processing unit can perform many basic instructions, such as
addition or division, and mathematical operations, such as sine and
sqrt, allowing to reach 8 double precision operations per cycle (16
single precision). The unit also supports Fused Multiply-Add (FMA)
operations which are typically counted as two operations for
benchmarking purposes. Therefore, the peak performance of the 5110P
card is 1 Tflop/s in double precision and 2 Tflop/s in single
precision. If FMA can not be used and half of these number can be
achieved.


\subsection{Bringing the data in vector register}

As explained in Section~\ref{sec:model}, we expect to achieve a performance of
about 210Gflop/s in single precision in the best case. We will focus
on the single precision case with 4 vectors and 4 matrices. But
similar techniques apply for other combinations. This target
performance represents 10\% of the peak performance of the
architecture which is impossible without an efficient vectorization of
the kernel. Indeed, to reach such a performance, a Fused Multiply-Add
instruction on fully loaded registers must be executed at most every
10 cycles.

We discuss how the implementation of the v4m4 kernel works using
vectorial instruction. The code is given for reference in
Figure~\ref{code:mat_mul}. (We will show in Section~\ref{sec:expe}
that an implementation relying on compiler vectorization does not lead
to desirable performance, which confirms the results
of~\cite{Saule13-ARXIV}.) The registers in the MIC architecture are
512 bits wide and can store 16 floats or integers. (In comparison to
OpenCL or CUDA, one can think of these registers as a warp.) The end
goal is to be able to load and format the non zero values $a,b,c,d$
and the vector entries $p,q,r,s$ in two vector registers to be able to
apply Fused Multiply-Add on them, which is depicted in
Figure~\ref{fig:tensor_product}. Bringing the data in the vector
register as depicted in the figure is the difficult part that we now
explain. A thread will perform the multiplications one row at a time,
and parallelism is achieved by giving blocks of rows to each core
using an OpenMP construct.

\begin{figure*}
  \centering
\subfigure[Source code]{\includegraphics[clip=true,trim=4.5cm 11cm 4cm 4.2cm,width=.6\linewidth]{code.pdf}}%
\subfigure[Content of the vector registers]{\hspace{-0.07\linewidth}\includegraphics[width=.5\linewidth]{figures/code-annote.pdf}}

\caption{Code snippet of the multiplication of 4 vectors with 4
  matrices. When a line changes the content of a vector register its
  new content is shown on the right. The effects of swizzling and
  permutations are highlighted using a color code.}
\label{code:mat_mul}

\end{figure*}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/tensor_product.pdf}
  \caption{Structure of the vector registers to perform an efficient
    tensor scalar product. See Figure~\ref{fig:mat_struct} for
    reference.}
  \label{fig:tensor_product}
\end{figure}

The core of the technique is the use of swizzling and permutation
features of the vectors in the Intel MIC architecture. A vector of 512
bits is composed of 4 lanes (sometimes called channels) of 128 bits
which are made of 4 elements of 32 bits. A swizzling operation allows
to reorder the elements within each lane. While a permutation allows to
change the orders of the lanes. Figure~\ref{fig:swizzling_permutation}
illustrates these two possibilities. Both permutation and swizzling
support common reordering and broadcast. In the code, the ``permute''
function needs to use the {\tt \_mm512\_permute4f128\_epi32}
instruction which is only defined on integers and therefore typecast
are necessary to convert to a vector to/from the integer type.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/swizzling_lanes.pdf}
  \caption{Swizzling enables to reorder or broadcast elements within
    each 128 bit group. Permutation enables to reorder or broadcast
    whole 128 bit groups.}
  \label{fig:swizzling_permutation}
\end{figure}

All the operations that loads data from the memory into a register
allows to transfer 512 bits, so 16 floats at a time. A single non zero
in the matrix will cause 16 operations to be performed, but it only
uses 4 floats from the matrix and 4 floats from the {\tt x} vector. So
we need to load at once the data for 4 non zeroes in the matrix,
representing 16 floats from the matrix and 16 floats from the vectors
at once. (The number of non zeroes on a row in our application is
always a multiple of 4, if it was, we would need to add some explicit
zeroes in the data structure to pad it.)

The values from the non zero element in the matrix are the simplest to
load. One can load the 16 floating point values for 4 non zeroes of
the matrix at once. The 4 floats coming from each matrix four 4
consecutive non zeroes are naturally grouped within each lane of the
SIMD vector. One can use a swizzle operation to broadcast the 4
elements of a single matrix to take the whole vector (values
$A_h^1,A_h^2,A_h^3,A_h^4$ are broadcasted in Figure~\ref{code:mat_mul}).

Loading the entries from the {\tt x} vector is a little bit more
complicated. First the columns pointers of 4 different non zeroes are
loaded in a vector register and the values are distributed one per
lane of the vector register using an {\tt unpack}
operation\footnote{Notice that there are two calls to unpack for the
  LSB and MSB. Both are mandatory even if one is known not to be
  useful according the documentation of the hardware}. (The column pointers are named $j,k,l,m$ in Figure~\ref{code:mat_mul}.) They replicated
so that each fill a vector lane using a swizzle operation. Then each
value is multiplied by 4 (because there are 4 vectors) and the offsets
$(3,2,1,0)$ is added each lane to obtain the correct index of the
elements of the {\tt x} vector within each lane. Then a {\tt gather}
operation is performed bring the 16 entries of the {\tt x} vector into
the SIMD register. At this point we have a SIMD register where each
lane is the four vector entries for a non zero element. Using a
broadcast permutation, one can replicate an entry of the four vectors
accross each lane.



\section{Experimental Validation}
\label{sec:expe}

\subsection{Instances}


\def\ww{.13\textwidth}
\begin{figure*}[tbh]
  \begin{center}
    \subfigure[Supercompact.]{%
      \label{fig:supercompact}
      \includegraphics[width=\ww]{figures/supercompact_matrix-crop.png}}%
    \subfigure[Compact.]{%
      \label{fig:compact}
      \includegraphics[width=\ww]{figures/compact_matrix-crop.png}}%
    \subfigure[Random.]{%
      \label{fig:random}
      \includegraphics[width=\ww]{figures/random_matrix-crop.png}}%
    \subfigure[2D, no RCM.]{%
      \label{fig:rbf2dnorcm}
      \includegraphics[width=\ww]{figures/kd-tree-2d-norcm-crop.png}}%
    \subfigure[2D, RCM.]{%
      \label{fig:rbf2drcm}
      \includegraphics[width=\ww]{figures/kd-tree-2d-rcm-crop.png}} 
    \subfigure[3D, RCM.]{%
      \label{fig:rbf3dnorcm} 
      \includegraphics[width=\ww]{figures/kd-tree-3d-norcm-crop.png}}%
    \subfigure[3D, RCM.]{%
      \label{fig:rbf3drcm}
      \includegraphics[width=\ww]{figures/kd-tree-3d-rcm-crop.png}}%
  \end{center}
  
  \caption{Different sparsity distributions. In all cases, there are
    32 nonzeros per line and 300 rows. The last four matrices
    corresponds to derivative stencils in 2D and 3D RBF-FD
    calculations.}
  \label{fig:spy_plots}
\end{figure*}

To perform our experiments and analysis, we use multiple kinds of
matrices which are presented in Figure~\ref{fig:spy_plots}. The
Supercompact matrices (Figure~\ref{fig:supercompact}) have been
generated to only have non zero elements in the first 32 columns of
the matrix. That way we ensure the simplest form of multiplication,
the amount of elements from the {\tt x} vector data transfered from
the memory will be minimum. The Compact matrices
(Figure~\ref{fig:compact}) are generated to have 32 non zeroes per row
centered around the diagonal and it represents the ideal case for most
application relying on sparse matrix vector multiplication. The Random
matrices (Figure~\ref{fig:compact}) see their non zero element
randomly (uniformly) distributed in the matrix, they represent the
worst case scenario for cache based architecture where the cache
reutilisation is the lowest from one row to the next.

The other two types of matrices are used in derivative stencils in 2D
and 3D RBF-FD calculations (Figure~\ref{fig:rbf2dnorcm} pictures a
$32$-points stencil of a $23^2$ 2D grid and
Figure~\ref{fig:rbf2dnorcm} shows a $32$-points stencil of a $8^3$
grid). We also apply Cuthill-Mckee reordering to all the matrices This
ordering technique is designed to reduce the distance between the non
zeroes and the diagonal, hopefully allowing a better cache
reutilisation. The reordered version of these two matrices can be seen
in Figure~\ref{fig:rbf2drcm} and in Figure~\ref{fig:rbf3drcm}.


\subsection{Bandwidth}

\begin{figure*}
\centering
  \subfigure[
    Entries with "cpp" denote cases where
    coding was performed without MIC vector
    instructions.]{\includegraphics[width=.40\linewidth]{figures/test1_readwrite.pdf}\label{fig:band_rw}}
%
  \hspace{.3in}
%
  \subfigure[
    The speed on the CPU matches the speed with vector instructions.
    All memory is aligned on 32
    bytes.]{\includegraphics[width=.40\linewidth]{figures/host_test1_readwrite_no_temporal_hint.pdf}\label{fig:read_write}}

\caption{Bandwidth performance on the host under idealized conditions as a function of matrix row size.
  Entries with "cpp" denote cases where coding was performed without MIC vector intructions. 
\todo{not 100\% sure what this plots are.}}
\end{figure*}

\begin{figure*}
\centering
  \subfigure[ xxx ] 
  {\includegraphics[width=.20\linewidth]{figures/test1_gather.pdf}\label{fig:band_gather}} \\

  \subfigure[The greater
    speed of the cpp version is obtained throught the use of
    \ttt{\#Ivdep} {\em and\/} \ttt{\_\_assumed\_aligned}. All memory
    is aligned on 64
    bytes.]{\includegraphics[width=.20\linewidth]{figures/test3_gather.pdf}\label{fig:band_gather_ivdep}}
\end{figure*}
%

\subsection{Computations}

\begin{figure*}
  \centering
  \subfigure[Performance of $y=Ax$ on the MIC. Squares: base 1/1 case,
    solid circles: 4/4 case implemented in C++, solid triangles: 4/4
    case implemented with MIC vector instructions. $-O3$ compilation
    options. Each symbol has two cases: with (red) and without blue) 
    with and without Reverse Cuthill-McKee. The grid is $96^3$.]
    {\includegraphics[width=.40\linewidth]{figures/mic_performance_nb_threads.pdf}\label{fig:perf_mic}}
%
\hspace{.4in}
%
  \subfigure[Performance on the host on a sparse matrix with $64^3$ rows. Manual
    vectorization brings significant improvement there as
    well.]{\includegraphics[width=.40\linewidth]{figures/64x64x64.pdf}\label{fig:perf_host}}
  \caption{Vectorization matters in practice. Differences in BW
    translate in different performance.\todo{this two plots should ``correlate'' }}
\end{figure*}


\begin{figure*}[t]
  \centering
  \subfigure[Supercompact and random.]{\includegraphics[width=.48\linewidth]{figures/random_supercompact.pdf}}
  \subfigure[Compact and RBF-FD]{\includegraphics[width=.48\linewidth]{figures/rbf_compact.pdf}}

  %\subfigure[Supercompact]{\includegraphics[width=.48\linewidth]{figures/supercompact_max_perf.pdf}}
  %\subfigure[Compact]{\includegraphics[width=.48\linewidth]{figures/compact_max_perf.pdf}} \\
  %\subfigure[Random]{\includegraphics[width=.48\linewidth]{figures/random_max_perf.pdf}}
  %\subfigure[RBF 3D]{\includegraphics[width=.48\linewidth]{figures/rbf_max_perf_run1.pdf}}
  \caption{Performance of "best code" accross instance types.}
\end{figure*}


%<<<<<<< HEAD
%\begin{figure}
%  \centering
%  \includegraphics[width=.45\textwidth]{figures/plot_for_natasha_ncar_end_of_year.pdf}
%
%  \caption{Performance on a MIC of derivative computation for RBFFD
%    with 884,736 nodes distributed quasi-randomly in a cube. Single
%    derivative of a single function (base case, triangle) and four
%    derivatives of four functions (multi case, circles). No bandwidth
%    reduction (blue) and Reverse Cuthill-McKee (red).\todo{should we have a similar one for the host?}}
%\end{figure}
%
%\begin{figure*}
%\centering
%\subfigure[Best case.]{\includegraphics[width=.33\linewidth]{figures/flops_to_bytes_best-crop.pdf}} %\label{fig:ratio_best}}
%\subfigure[Worst case.]{\includegraphics[width=.33\linewidth]{figures/flops_to_bytes_worst-crop.pdf}}%\label{fig:ratio_worst}}
%\subfigure[Worst case (No cacheline effects)]{\includegraphics[width=.33\linewidth]{figures/flops_to_bytes_no_cache-crop.pdf}}%\label{fig:ratio_nocache}}
%\caption{Ratio of flops to bytes; (a) simple count of flops and bytes that require transfer from memory;(b) best and (c) worst
%case taking into account that bringing one element of $x$ into memory brings along the remainder of the cache line.}
%\label{fig:ratio_bytes_flops}
%\end{figure*}
%=======
%\begin{figure}
%  \centering
%  \includegraphics[width=.45\textwidth]{figures/plot_for_natasha_ncar_end_of_year.pdf}
%
%  \caption{Performance on a MIC of derivative computation for RBFFD
%    with 884,736 nodes distributed quasi-randomly in a cube. Single
%    derivative of a single function (base case, triangle) and four
%    derivatives of four functions (multi case, circles). No bandwidth
%    reduction (blue) and Reverse Cuthill-McKee (red).\todo{should we have a similar one for the host?}}
%\end{figure}
%
%>>>>>>> 619115cea6da9466f09a3b01456eacf7991606e2

Derivatives in an RBFFD formulation are expressed as a sparse
matrix/vector multiplication (SpMV). Using the full 61 nodes of a MIC
processor, and all four threads of each node, we achieve lowly 18
Gflops when calculating a single vector of a single
function. Bandwidth reduction has minimal effect since the cache is
large enough to store both the derivative matrix and the solution
vector. We increase the number of computations per byte transferred by
calculating four different derivatives of four different
functions. With bandwidth reduction, we speed the calculation by a
factor of 8, and achieve upwards of 140 Gflops.


\section{Conclusion}
\label{sec:ccl}

In the previous sections, we have explored in the practical
implementation on a MIC architecture of multiple derivative operators
acting on multiple vectors within the context of RBF-RD. Each
derivative has an associated sparse matrix with a fixed number of
non-zeros on each row. While computing a single derivative of multiple
functions is rather common (vectorization occurs over the vectors), we
accelerate the algorithm further by considering multiple derivatives
with corresponding matrices with identical adjacency graphs. We
specialize to four matrices and four derivatives, with 16 outputs,
computed as a sum of outer products.

Using a matrix with $64^3$ (or $96^3$ rows, we achieve a speedup of
xxx relative to the base case of one derivative and one function (1/1
case), relative to a potential speedup of 16. We have achieved a
speed of 100 Gflop/s on 60 cores using 240 threads, or a speed of 61\% of
maximum possible performance. The maximum possible speed for the 1/1
case is around 40 Gflop/s, of which we achieve 20 Gflop/s. On matrices
that exceed the cache, a reverse Cuthill-McKee is applied to reduce the
bandwidth, with an associated speedup of 30\% up to 135 Gflop/s. Our
optimal implementation makes use of the (as yet unreleased) IMCI MIC instruction set 
%\todo{that's not AVX on MIC, not sure what the name is}, 
which includes a number of swizzling and channel swapping operations, for an
extremely efficient tensor product implementation. A straightforward
implementation without vectorization runs at around 35 Gflops, or 16\% of peak
performance. RCM has no effect because the number of floating point
operations is excessive (the algorithm does not fill the vector
registers to capacity).

In a future work, we will examine the effect of larger stencil sizes,
double precision, and the actual cost of computing the derivatives
within a fluid simulation using RBF-FD.

\section*{Acknowledgment}
The first and last authors acknowledge NSF funding under NSF grant DMS-\#0934331 (FSU). 
NCAR is sponsored by the National Science Foundation. N. Flyer      
acknowledges support of NSF grant DMS-0934317.


\bibliographystyle{plain} %{IEEEabrv,paper.bib}
\bibliography{paper, bollig}
\end{document}


