\documentclass[10pt, conference, compsocconf]{IEEEtran}
\input{support/macros} % color is defined in macros or misc_mac
\input{support/misc_mac}
\input{support/setupicase}


\def\red#1{\textbf{\textcolor{red}{#1}}}
\def\blue#1{\textbf{\textcolor{blue}{#1}}}
\def\qes#1{{\blue{*** For Erik: #1 ***}}}
\def\es#1{{\blue{*** For Erik: #1 ***}}}
\def\ge#1{{\red{*** For Gordon: #1 ***}}}
\def\ttt#1{{\tt #1}}
\def\bold#1{{\bf #1}}

\def\qes#1{}
\def\es#1{}
\def\ge#1{}
\usepackage{morefloats}


\usepackage{graphicx}
\usepackage[tight,footnotesize]{subfigure}
\usepackage{fixltx2e}
\usepackage{url}
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Sparse Matrix Vector Multiplication with Multiple vectors and Multiple Matrices on the
   MIC Architecture}


\author{\IEEEauthorblockN{Gordon Erlebacher\IEEEauthorrefmark{1},
Erik Saule\IEEEauthorrefmark{2}, Natasha Flyer\IEEEauthorrefmark{3}, 
and Evan Bollig\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Department of Scientific Computing, 
Florida State University, Tallahassee, FL 32306-4120\\
Email: gerlebacher@fsu.edu}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Department of Computer Science, University of North Carolina at Charlotte\\
Email: erik.saule@uncc.edu}
\IEEEauthorblockA{\IEEEauthorrefmark{3}Computational and Information Systems Laboratory, UCAR \\
Email: 
flyer@ucar.edu}
\IEEEauthorblockA{\IEEEauthorrefmark{4}Department of Scientific Computing, Florida State University\\
Email: bollig@gmail.com}}
\maketitle


\begin{abstract}
In this paper, we develop an efficient scheme for the calculation of derivatives within the context
of Radial Basis Function Finite-Difference (RBFFD). RBF methods express functions as a linear
combination of radial basis functions on an arbitrary set of nodes. The Finite-Difference component
expresses this combination over a local set of nodes neighboring the point where the derivative is sought.
The derivative at all points takes the form of a sparse matrix/vector multiplication (spmv).

In this paper, we consider the case of local stencils with the number of nodes at each point and encode the
sparse matrix in ELLPACK format. We increase the number of operations relative to memory bandwidth by
calculating four derivatives of four different functions, or 16 different derivatives. We demonstrate
a novel implementation on the MIC architecture, taking into account its advanced swizzling and channel
interchange features. We present benchmarks that show an almost order of magnitude increase in speed
compared to efficient implementations of a single derivative. We explain the results through consideration
of operation count versus memory bandwidth.
\end{abstract}

\begin{IEEEkeywords}
OpenMP; MIC; spmv; sparse matrix; Radial Basis Function;
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle



\section{Introduction}
\cite{Bell08, Vuduc05, Nishtala07, Stock12-TACC, Cuthill69, cramer2012openmp, Buluc2009_SPAA, Buluc11, Im01, Mellor-Crummey04, Nishtala07, Saad94sparskit, Williams07}
\cite{Temam:1992:CBS:147877.148091}
\cite{DBLP:journals/ijhpca/ShantharamCR11,
conf/ppsc/Toledo97, Liu:2013:ESM:2464996.2465013, Molka:2009:MPC:1636712.1637764,%
DBLP:journals/corr/abs-1101-0091, conf/ipps/BulucWOD11, conf/ipps/KreutzerHWFBB12,%
kumar2012accelerating, journals/concurrency/VazquezFG11}


SpMV is an important kernel for lots of stuff. So improving the
performance of SpMV has caputred the interest of many researcher.

The main challenge that is faced to improve good performance for SpMV
is that the operation are conducted using memory location that are
irregular and often unpredictible. That make that the kernels are
mostly memory bound and there is a significant instruction overhead
per flop.

Common improvement techniques such as register blocking, bandiwdth
reduction (matrix reordering), partitioning to fit in cache or TLB
have impacts which are very dependent on the matrix and overall do not
lead to dramatic improvement. Assuming register blocking does not
apply well to the matrix at hand (which is true for most matrices),
there is about 8 bytes of the matrix to move in per non zero (assuming
single precision); each nonzero requires two floating point operations
leading to a flop-to-byte ratio of at most $\frac{1}{4}$. This limits
the obtained performance to at most a quarter of the bandwidth of the
architecture wasting a lot of potentially useful cycles. The commonly
used techniques are mostly designed to reach that bound rather than
overcome it.

Fortunately that fate is not inevitable. One solution would be to pair
multiple component of an application to schedule a more instruction
intensive kernel simultaneously with SpMV, relying on some hardware
threading capabilities, such as HyperThreading, to reduce the cycle
wastage. However most ot the applications that use SpMV do not
typically have an instruction intensive kernel to run simultaneously.

An other solution, and the one we pursue in this paper, is to compute
multiple SpMV at once on matrices that have the same sparsity
patterns. Obviously not all the applications have such a property. But
important classes of applications such as graph
recommendation~\cite{}, eigensolving~\cite{} and the computation of
derivative of Radial Basis Functions(RBF)~\cite{} can use multiple
SpMVs simultaneously. In this paper in particular, we investigate the
case of the derivative of RBFs where four derivatives of four
different function is expressed as the multiplication of four vectors
by four matrices with identical sparsity patterns leading to the
simultaneous execution of 16 SpMVs at a time.

To perform our analysis, we focus our attention on the improvement
that we can achieve on the Intel Xeon Phi processor. It follow the
Many Integrated Core (MIC) architecture, which has a significant
memory bandwidth and peak flop throughput thanks to its 512-bit large
SIMD registers. The Xeon Phi processor has been shown to be promising
for sparse linear algebra compared to more classical CPU or GPU
architecture~\cite{}.

In Section~\ref{sec:rbf} we present the computation of RBFs and how it
can be expressed 16 multiplication of 4 vectors by 4 sparse matrices
with a common sparsity pattern. Section~\ref{sec:model} presents an
estimation of the instruction intensity of various form of the
computations and we show that a 5- to 6- fold improvement can be
expected when computing the 16 multiplications simultaneously and
reach a total of about 200 Gflop/s. This performance represents
approximatively 10\% of the available flop/s of a Knight Corner
coprocessor. Therefore, it is necessary to have implementations that
perform the computation in as little amount of instructions as
possible. We describe in Section~\ref{sec:impl} the details of the MIC
architecture and how to use specialized load, store, swizzle and
permutation instruction to efficiently bring the data in the vector
registers to be processed. Section~\ref{sec:expe} gives some
experimental result about the amount of bandwidth that can be achieved
depending on how the spmv kernel is written and the actual performance
of the various kernel on multiple classes of matrices some generated
for analysis purpose and some extracted from and RBF application. A
performance of xxx GFlop/s is achieved on real scenario. Concluding
remarks and perspectives are provided in Section~\ref{sec:ccl}.

\section{Derivatives of Radial Basis Functions}
\label{sec:rbf}


\section{Modelization of the Potential Improvements}
\label{sec:model}

\section{Efficient Implementation on the Intel Xeon Phi processor}
\label{sec:impl}

\section{Experimental Validation}
\label{sec:expe}

%\input{problem_statement}
%\input{intel_phi}
%\input{tables_and_figures}
%\input{bandwidth}
%\input{notation}

\section{Conclusions and Perspectives}
\label{sec:ccl}

The conclusion goes here. this is more of the conclusion

\section*{Acknowledgment}


The authors would like to thank...
more thanks here


\bibliographystyle{plain} %{IEEEabrv,paper.bib}
\bibliography{paper, bollig}
\end{document}


