\documentclass[10pt,conference,compsocconf]{IEEEtran}
\input{support/macros} % color is defined in macros or misc_mac
\input{support/misc_mac}
\input{support/setupicase}


\def\red#1{\textbf{\textcolor{red}{#1}}}
\def\blue#1{\textbf{\textcolor{blue}{#1}}}
\def\qes#1{{\blue{*** For Erik: #1 ***}}}
\def\es#1{{\blue{*** For Erik: #1 ***}}}
\def\ge#1{{\red{*** For Gordon: #1 ***}}}
\def\ttt#1{{\tt #1}}
\def\bold#1{{\bf #1}}

\usepackage{soul}
\usepackage{xspace}
\usepackage{color} 
\definecolor{darkgreen}{rgb}{0,0.5,0}
\usepackage[colorlinks=true,% 
  linkcolor=red,% 
  citecolor=darkgreen,%
  urlcolor=blue]{hyperref}


\newcommand{\todo}[1]{{\color{red}\textbf{\hl{#1}}\xspace}}

\def\qes#1{}
\def\es#1{}
%\def\ge#1{}
%\usepackage{morefloats}

\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}


\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
%\usepackage{fixltx2e}
\usepackage{url}
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Sparse Matrix Vector Multiplication with Multiple vectors and
  Multiple Matrices on the MIC Architecture}


\author{\IEEEauthorblockN{Gordon Erlebacher\IEEEauthorrefmark{1},
Erik Saule\IEEEauthorrefmark{2}, Natasha Flyer\IEEEauthorrefmark{3}, 
and Evan Bollig\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Department of Scientific Computing, 
Florida State University, Tallahassee, FL 32306-4120\\
Email: gerlebacher@fsu.edu}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Department of Computer Science, University of North Carolina at Charlotte\\
Email: esaule@uncc.edu}
\IEEEauthorblockA{\IEEEauthorrefmark{3}Computational and Information Systems Laboratory, UCAR \\
Email: flyer@ucar.edu}
\IEEEauthorblockA{\IEEEauthorrefmark{4}Department of Scientific Computing, Florida State University\\
Email: bollig@gmail.com}}
\maketitle


\begin{abstract}
In this paper, we develop an efficient scheme for the calculation of
derivatives within the context of Radial Basis Function
Finite-Difference (RBFFD). RBF methods express functions as a linear
combination of radial basis functions on an arbitrary set of
nodes. The Finite-Difference component expresses this combination over
a local set of nodes neighboring the point where the derivative is
sought.  The derivative at all points takes the form of a sparse
matrix/vector multiplication (spmv).

In this paper, we consider the case of local stencils with the number
of nodes at each point and encode the sparse matrix in ELLPACK
format. We increase the number of operations relative to memory
bandwidth by calculating four derivatives of four different functions,
or 16 different derivatives. We demonstrate a novel implementation on
the MIC architecture, taking into account its advanced swizzling and
channel interchange features. We present benchmarks that show an
almost order of magnitude increase in speed compared to efficient
implementations of a single derivative. We explain the results through
consideration of operation count versus memory bandwidth.
\end{abstract}

\begin{IEEEkeywords}
OpenMP; MIC; spmv; sparse matrix; Radial Basis Function;
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
\cite{Bell08, Vuduc05, Nishtala07, Stock12-TACC, Cuthill69, cramer2012openmp, Buluc2009_SPAA, Buluc11, Im01, Mellor-Crummey04, Nishtala07, Saad94sparskit, Williams07}
\cite{Temam:1992:CBS:147877.148091}
\cite{DBLP:journals/ijhpca/ShantharamCR11,
conf/ppsc/Toledo97, Liu:2013:ESM:2464996.2465013, Molka:2009:MPC:1636712.1637764,%
DBLP:journals/corr/abs-1101-0091, conf/ipps/BulucWOD11, conf/ipps/KreutzerHWFBB12,%
kumar2012accelerating, journals/concurrency/VazquezFG11}

SpMV is an important kernel for lots of stuff. So improving the
performance of SpMV has caputred the interest of many researcher.

The main challenge that is faced to improve good performance for SpMV
is that the operation are conducted using memory location that are
irregular and often unpredictible. That make that the kernels are
mostly memory bound and there is a significant instruction overhead
per flop.

Common improvement techniques such as register blocking, bandiwdth
reduction (matrix reordering), partitioning to fit in cache or TLB
have impacts which are very dependent on the matrix and overall do not
lead to dramatic improvement. Assuming register blocking does not
apply well to the matrix at hand (which is true for most matrices),
there is about 8 bytes of the matrix to move in per non zero (assuming
single precision); each nonzero requires two floating point operations
leading to a flop-to-byte ratio of at most $\frac{1}{4}$. This limits
the obtained performance to at most a quarter of the bandwidth of the
architecture wasting a lot of potentially useful cycles. The commonly
used techniques are mostly designed to reach that bound rather than
overcome it.

Fortunately that fate is not inevitable. One solution would be to pair
multiple component of an application to schedule a more instruction
intensive kernel simultaneously with SpMV, relying on some hardware
threading capabilities, such as HyperThreading, to reduce the cycle
wastage. However most ot the applications that use SpMV do not
typically have an instruction intensive kernel to run simultaneously.

An other solution, and the one we pursue in this paper, is to compute
multiple SpMV at once on matrices that have the same sparsity
patterns. Obviously not all the applications have such a property. But
important classes of applications such as graph
recommendation~\cite{}, eigensolving~\cite{} and the computation of
derivative of Radial Basis Functions(RBF)~\cite{} can use multiple
SpMVs simultaneously. In this paper in particular, we investigate the
case of the derivative of RBFs where four derivatives of four
different function is expressed as the multiplication of four vectors
by four matrices with identical sparsity patterns leading to the
simultaneous execution of 16 SpMVs at a time.

To perform our analysis, we focus our attention on the improvement
that we can achieve on the Intel Xeon Phi processor. It follow the
Many Integrated Core (MIC) architecture, which has a significant
memory bandwidth and peak flop throughput thanks to its 512-bit large
SIMD registers. The Xeon Phi processor has been shown to be promising
for sparse linear algebra compared to more classical CPU or GPU
architecture~\cite{}.

In Section~\ref{sec:rbf} we present the computation of RBFs and how it
can be expressed 16 multiplication of 4 vectors by 4 sparse matrices
with a common sparsity pattern. Section~\ref{sec:model} presents an
estimation of the instruction intensity of various form of the
computations and we show that a 5- to 6- fold improvement can be
expected when computing the 16 multiplications simultaneously and
reach a total of about 200 Gflop/s. This performance represents
approximatively 10\% of the available flop/s of a Knight Corner
coprocessor. Therefore, it is necessary to have implementations that
perform the computation in as little amount of instructions as
possible. We describe in Section~\ref{sec:impl} the details of the MIC
architecture and how to use specialized load, store, swizzle and
permutation instruction to efficiently bring the data in the vector
registers to be processed. Section~\ref{sec:expe} gives some
experimental result about the amount of bandwidth that can be achieved
depending on how the spmv kernel is written and the actual performance
of the various kernel on multiple classes of matrices some generated
for analysis purpose and some extracted from and RBF application. A
performance of xxx GFlop/s is achieved on real scenario. Concluding
remarks and perspectives are provided in Section~\ref{sec:ccl}.

\section{Derivatives of Radial Basis Functions}
\label{sec:rbf}



%In the theory of Radial Basis Functions Finite-Difference (RBFFD),
%derivatives of a function $f(\rvec)$ at node $i$ are expressed as a
%linear combination of the function values at the stencil center and
%the nodes connected to node $i$ (Figure~\ref{fig:rbf_stencils}). Thus
%$y$ is a discrete derivative of the vector $x$.

In this paper, we propose yet a new idea, that is applicable to the
world of Finite-Difference Radial Basis functions (RBF-FD). A radial
basis functions refers to a set of shifted radially symmetric
functions $\phi_i(r) =\phi{||\xvec-\xvec_i||}$ centered at
$\xvec_i$\cite{}. Any function $f(\xvec)$ defined in a domain $\Omega$
can be expanded in this basis according to
\begin{equation}
f(\xvec) =  \sum_i w_i \phi_i(\xvec) \label{eq:rbf}
\end{equation}
% Wrong label eq:rbf in document 
Given function values at a set of discrete points $x_i$, one inverts
linear system (\ref{eq:rbf}) to obtain a set of weights $w_j$. Given a
set of $N$ RBF nodes, the matrix to invert is dense. Derivatives of
$f(\xvec)$ result from the differentiation of Eq. (\ref{eq:rbf}). For
very large problems, this is an computationally expensive since the
RBFs do not form an orthogonal basis, nor are they amenable to tensor
decomposition in the three coordinate directions. Rather, their
strength is the ability to randomly distribute nodes across complex
physical domains, and have an implementation that is independent of
dimensionality.

To alleviate the cost of a global approach, researchers \cite{} expand
$f(\xvec)$ in a local region of $x_i$. Thus,
$$
f(\xvec) =  \sum_{i=0}^{n_s} w_i \phi_i(\xvec) \label{eq:rbf}
$$
where one only considers a local set of $n_s$ neighbors of of point $x_j$. 
Finite-difference RBF expresses the derivative of $f(\xvec)$ at $x_j$ as a linear
combination of $f(\xvec)$ at the $n_s$ neighbors. Thus, 
$$
f = A w
$$ where $f$ is the vector of function values at the $n_s$ stencil
points (including the center point $x_j$. The matrix $A$ is a function
of $\phi_{ij}(\xvec) =\phi(||x_i-x_j||)$. This leads to the $n_s$
weight values $w = A^{-1} f$. The derivative of Eq. \ref{eq:rbf}
evaluated at $x_j$ gives
$$
   f'_j = \sum_{i=0}^{n_s} w_i \phi'_{ij}
$$
Substituting the vector of weights $w$ leads to 
$$
  f'_j = \sum_{i=0}^{n_s} \alpha_i f_i
$$
The derivative evaluation at all points of the domain $\Omega$, notwithstanding boundary conditions, takes the form
$$
  y = A x
$$ 
where $x$ is the souce vector of function values, and $y$ is the
vector of derivative values. The matrix $A$ is called a derivative
matrix. In the current litterature, the number of stencil points is a
fixed value, independent of the node under consideration\cite{}. The
computation of a single derivative has been reduced to a SpMV, where
each row has $n_s$ nonzeros. Because the sparsity of each row is
constant, ELLPACK~\cite{} becoes the most appropriate matrix
compression scheme\cite{}. In practice, $n_s=32$ in two-dimensional
flows and 64 or 100 for three-dimensional flows. These numbers are
similar to what is used in finite-element codes.

Many problems in fluid dynamics and in the geosciences require the solution to
transport equations of the form
$$
\pf{Q}{t} = f(Q,Q_x, Q_y, Q_z, \Laplacian{Q})
$$ 
where $Q$ is a vector of unknowns (3 components of velocity,
pressure, temperature). Thus, it is often necessary to compute the $x$
derivative of multiple functions, typically four for the Euler
equations, or five for the Navier-Stokes equations. Multiple
right-hand sides transform a SpMV into a SpMM (Sparse Matrix/dense
Matrix multiplication), which improves register utilization and
decreases cache misses by vectorizing over the multiple source
vectors. Further improvements are possible by recognizing that
different derivative matrices (x,y,z and Laplacian for example), have
the same sparsity distribution; only the weight change.  Thus,
alternative to computing a derivative of multiple functions, we can
calculate multiple derivatives of a single functions. The increased
memory bandwidth due to an increase in the number of derivaive
matrices is offset by better cache utilization, leading to an overall
benefit.


**** PUT ELSEWHERE OR ELIMINATE? ****

When solving a one dimensional system of PDEs, one might require an
$x$ derivative of multiple functions. For example, the Euler equations
require the $x$ derivative of the three components of velocity and
pressure. In this case, there are $n_v=4$ vectors $x^k$,
$k=0,\cdots,3$.

Thus for each vector element $y_i$, we compute $y_i = \sum_j A_{ij}
x_j$. If $A_i$ is row $i$ of $A$, $y_i$ is simply the dot product $A_i
x$. The next level of generality is to consider $n_v$ vectors $x^k$,
$k=0,\cdots,n_v-1$. Whatever the spmv implementation, one achieves
improved performance if the matrix formed from the columns $x^k$ are
stored in row major order. Thus, $x^0_0,x^1_0,\cdots,x^{n_v-1}_0$, are
stored in consecutive memory location. The random access of the
elements of $x$ is thus reduced. Maximum efficiency is achieved when
$n_v=16$ floats or $8$ doubles, given that cache lines take 64
bytes. We will benchmark this case, labeled $Svn$, where $n$ refers to
the number of vectors (Iv4 uses four vectors). The $S$ refers to singe
precision. A double precision run is labelled $Dvn$.

Alternatively, when solving a PDE, one might require derivatives of a
given scalar function with respect to coordinate directions $x$, $y$,
$z$. Second order operators of often required, such as a second
derivative with respect to $x$ or a Laplacian operator. In the RBFFD
formulation, on can compute different derivatives using the same
stencil, but with different weights. In other words, the adjacency
matrix that corresponds to $A$ remains constant, but the matrix
elements of $A$ change with the particular derivative.  In this case,
label with a superscript $l$ the particular matrix $A^k$. Since the
adjacency matrix is assumed invariant, there is only need for a single
matrix \ttt{C{ij}}. In Ellpack format, each row is of constant size
(the number of nonzeros per row of $A$. $C_{ij}$ is the column number
that locates the $j^{th}$ nonzero in row $i$ of $A$.


\todo{Must rewrite the above to make it more focused on the
  application. Remove the architectural/implementation details such as
  using ELLPACK. Let's focus on on why this computation is important
  and how does the 16 multiplication appears in the equations.}

\def\wide{2.5in}
\newfig{figures/rbf_stencils.pdf}{\wide}{RBFFD Stencils. Each node of the mesh is connected to $n_z-1$ stencil nodes in addition to itself. In the figure, node $A$ is connected to $B$, but $B$ is {\em not\/} connected to $A$. Thus adjacency graph of $A$ is not-symmetric. This must be corrected when reducing the bandwidth of $A$ with a Cuthill\-McKee algorithm, which assumes symmetry.}{fig:rbf_stencils}

\newfig{figures/matrix_structure.pdf}{\wide}{Matrix Structure.}{fig:mat_struct}


\section{Modelization of the Potential Improvements}
\label{sec:model}

We saw in the previous section that one can express the RBF problem as
a multiplication of four matrices by four vectors. We present here an
estimation of the variation on the flop intensity of the computation
and its impact on the expected performance of the
application. Relevant notations are given in Table~\ref{tab:not}.

\begin{figure}
  \begin{center}
    \scalebox{.9}{
      \begin{tabular}{|c|l|}
        %\hline
        %& & \\
        \hline
        $b_i$ & number of bytes per index \\
        $b_x$ & number of bytes per value \\
        $n_z$ & number of nonzeros per row $A$ \\
        $n_r$ & number of column/rows of $A$ \\
        $n_c$ & total number of non-zero\\
        $n_v$ & number of {\tt x} vectors \\
        $n_m$ & number of matrices \\
        $s_M$ & size of the matrice(s) in bytes\\
        $s_x$ & size of the {\tt x} vector(s) in bytes\\
        $s_y$ & size of the {\tt y} vector(s) in bytes\\
        \hline
        $cl$    & size of a cache line in bytes\\
        $b_{wT}$ & number of bytes written to memory  \\
        $b_{rT}$ & minimum number of bytes read from memory  \\
        $b_T$   & minimum number of bytes transferred  \\
        $B_{rT}$ & maximum number of bytes read from memory \\
        $B_T$   & maximum number of bytes transferred  \\
        $O$     & number of floating point operations \\
        $I_b$   & maximal computational intensity\\
        $I_w$   & minimum computational intensity\\        
        \hline
      \end{tabular}
    }
  \end{center}
  \caption{Notations}
  \label{tab:not}
\end{figure}

Each vector in the problem is of dimension $n_r$ and each dimension
tasks $b_x$ bytes. There are $n_v$ {\tt x} vectors and $n_v n_m$ {\tt
  y} vectors, which lead to the size of the {\tt x} and {\tt y} vectors:
$$s_x = n_v b_x n_r$$ $$s_y = n_v n_m b_x n_r$$

The matrix is composed of $n_r$ rows and columns with $n_z$ non-zeros
per row leading to a total of $$n_c = n_r n_z$$ non-zero entries in
the matrix. Each of these non-zero entries has one index of size $b_i$
and $n_m$ values of size $b_x$. The matrices have a total size
of $$s_M = n_c (b_i + b_x n_m) = n_r n_z (b_i + b_x n_m)$$

If we assume an algorithm where the rows are processed one after
the other, the amount of memory written is precisely the
size of the {\tt y} vector. (This assumption removes the possibility of
cache partitioning techniques.) $$b_{wT} = s_y = n_v n_m b_x n_r$$

The amount of data read from memory depends highly on both the algorithm's
execution path, and on %algorithm execute, 
how the matrix is structured. But in the best case
both the matrix $A$ %is read a single time 
and the source vector {\tt x} are read once
%read a single time 
from the main memory. (We assume that all the elements of $\tt x$ are involved
in the SpMV.)  %is used is reasonnable since only oddly shaped matrix will
%left a dimension of the vector unused.) 
Thus, $$b_{rT} = s_M + s_x = n_r n_z
(b_i + b_x n_m) + n_v b_x n_r$$
 $$b_T = b_{rT} + b_{wT} =  n_r n_z (b_i + b_x n_m) + n_v b_x n_r (1 + n_m)$$

Notice that there is no reason for a piece of the matrix to be read
multiple times. But assuming that each element of the {\tt x} vector
is read a single time is a strong assumption. If using a single core,
it assumes that either the cache of the architecture can store the full
{\tt x} vector or that the matrix is sufficiently well structured 
causing no cache trashing. If using multiple core, this assumes that
no element of the {\tt x} vectors will be used by multiple
cores. \cite{Saule12} showed that there is very little cache trashing
in practice, but it showed that having elements of the vectors used by
multiple cores can have a significant impact on the performance
(growing with $n_v$).

On the other hand, in the worst case, every time the {\tt x} vector is
accessed, the value needs to be transfered from memory again. So in
total, there are as many transfers as the number of non-zeros in the
matrix. Note however that most architectures cannot read memory a single 
byte at a time. Instead, a minimum number of bytes,equal to the size o a cacheline, 
are transfered at once. 
%multiple bytes, with each chunk equal to the data do not allow to read the memory
%one byte at a time but always in a set of multiple bytes. On most
%cached architecture, that 
%the minimum size corresponds to the size $cl$ of
%a cacheline (typically 64 bytes). 
When there are multiple
vectors, each non-zero element uses $n_v$ consecutive entries. The
worst case number of bytes read and transfered are 
$$B_{rT} = s_M + n_c cl \ceil{\frac{n_vb_x}{cl}} $$ 
$$B_T = n_v n_m b_x n_r + n_r n_z \left ( b_i + b_x n_m +  cl \ceil{\frac{n_vb_x}{cl}} \right)$$

In SpMV, each non-zero of the matrix requires two floating point
operations: one for performing the multiplication and one for
accumulating the result row-wise. Here we are dealing with $n_v n_m$
simultaneous SpMVs and the number of floating point operations is
$$O = 2 n_v n_m n_c = 2 n_v n_m n_z n_r$$

The computation intensity is the amount of computations performed per
byte transfered. In the worst case and in the best case, we have
$$I_b = \frac{O}{b_T} = \frac{2 n_v n_m n_z}{n_z (b_i + b_x n_m) + n_v b_x (1 + n_m)}$$
$$I_w = \frac{O}{B_T} = \frac{2 n_v n_m n_z}{n_v n_m b_x + n_z \left ( b_i + b_x n_m +  cl \ceil{\frac{n_vb_x}{cl}} \right)}$$

In the special case $n_v=n_m=1$ in single precision
$$I_b = \frac{1}{4} \frac{ n_z }{ n_z + 1} \approx \frac{1}{4}$$
%$$I_w = \frac{n_z }{2  + 36  n_z} \approx \frac{1}{36}$$
$$I_w = \frac{n_z }{2  + (4+cl/2)  n_z} \approx \frac{1}{4+cl/2}$$

In the special case $n_v=n_m=4$ in single precision
$$I_b = \frac{32 n_z }{ n_z 20 + 80} \approx \frac{32}{20} = \frac{8}{5} $$
%$$I_w = \frac{32 n_z }{64 + n_z 84} \approx \frac{32}{84} = \frac{8}{21} $$
$$I_w = \frac{32 n_z }{64 + (20+cl) n_z} \approx \frac{32}{84} = \frac{8}{21} $$
The above formulas have been applied to MIC, which has a cache size $cl=64$ bytes.

\begin{table}
  \centering
  \begin{tabular}{|l|r|r|r|r|}
    \hline
                           & v1m1  & v1m4  & v4m1   & v4m4  \\
    \hline
    Best Single Precision  & 36.36 & 58.18 & 133.33 & 213.33\\
    Worst Single Precision &  4.15 & 14.20 &  16.55 &  55.81\\
    Best Double Precision  & 24.00 & 32.21 &  85.71 & 117.07\\
    Worst Double Precision &  3.93 & 11.88 &  15.58 &  46.15\\
    \hline
  \end{tabular}
  \caption{Estimation of performance in GFlop/s at 150GB/s for a
    matrix of $n_z = 32$}
\end{table}



\begin{figure*}
  \centering 
  \subfigure[Absolute performance ]{\includegraphics[width=.49\linewidth]{figures/gflops_peak.pdf}\label{fig:gflops_peak_perf}}
%
  \subfigure[Relative performance to base case using one matrix and one
    vector.]{\includegraphics[width=.49\linewidth]{figures/speedup_wrt_base.pdf}\label{fig:speedup}}
  
  \caption{Estimation of the reachable performance one can
      achieve using a device with a memory to computational units of
      150GB/s when varying the number of vectors and matrices}
  \label{fig:perf_predict}
\end{figure*}

\todo{conclude on the evaluation using the figure/table. Note that we
  base prediction on 150GB/s which is not the peak of saule12. Note
  that if you are in best in v1m1 you might go to bad v4m4, but if you
  are in bad v1m1, you will essentially stay there.}

\section{Efficient Implementation on the Intel Xeon Phi processor}
\label{sec:impl}

\subsection{Knights Corner}

In this work, we use an Intel Xeon Phi 5110P coprocessor. This card
has 8 memory controllers where each of them can execute 5 billion
transactions per second and has two 32-bit channels, achieving a total
bandwidth of 320GB/s aggregated across all the memory
controllers. There are 60 cores clocked at 1.05GHz. Their memory
interfaces are 32-bit wide with two channels and the total bandwidth
is 8.4GB/s per core. Thus, the cores should be able to consume 504GB/s
at most. However, the bandwidth between the cores and the memory
controllers is limited by the ring network that connect the cores and
the memory controller. Its precise bandwidth is unknown for this
precise card, but it would be coehrent that it is between 200GB/s and
250GB/s.

Each core in the architecture has a 32kB L1 data cache, a 32kB L1
instruction cache, and a 512kB L2 cache. The architecture of a core is
based the traditional Pentium architecture which have been extended to
64-bit. A core can hold 4 hardware contexts at any time. And at each
clock cycle, instructions from a single thread are executed. Due to
some hardware constraints, two hardware contexts must be used to reach
the peak instruction throughput in that architecture. As in the
Pentium architecture, a core has two different concurrent instruction
pipelines which allow the execution of two instructions per
cycle. However, only one vector or floating point instruction can be
executed at each cycle.

Most of the performance of the architecture comes from the vector
processing unit. Each core has 32 512-bit SIMD registers which can be
used for double or single precision, that is, either as a vector of 8
64-bit values or as a vector of 16 32-bit values, respectively. The
vector processing unit can perform many basic instructions, such as
addition or division, and mathematical operations, such as sine and
sqrt, allowing to reach 8 double precision operations per cycle (16
single precision). The unit also supports Fused Multiply-Add (FMA)
operations which are typically counted as two operations for
benchmarking purposes. Therefore, the peak performance of the 5110P
card is 1 Tflop/s in double precision and 2 Tflop/s in single
precision. If FMA can not be used and half of these number can be
achieved.


\subsection{Bringing the data in vector register}

As explained in Section~XXX, we expect to achieve a performance of
about 210Gflop/s in single precision in the best case. We will focus
on the single precision case with 4 vectors and 4 matrices. But
similar techniques apply for other combinations. This target
performance represents 10\% of the peak performance of the
architecture which is impossible without an efficient vectorization of
the kernel. Indeed, that means a Fused Multiply-Add instruction on  
fully loaded registers must be executed at most every 10 cycles.

Registers on the MIC are 512 bits wide, and can store 16 floats or 8
doubles. It is through these registers that the \#SIMD pragma is able
to achieve vectorization. (In comparison to OpenCL or CUDA, one can
think of these registers as a warp.)  We use the vector operations
presented in Figure~\ref{fig:MIC-vect-instr} in our code, which we'll
explain in the text.

\def\loadps{\ttt{\_mm512\_load\_ps}}
\def\loadds{\ttt{\_mm512\_load\_ds}}
\def\fmadps{\ttt{\_mm512\_fmad\_ps}}
\def\gatherps{\ttt{\_mm512\_i32gather\_ps}}
\def\swizzleps{\ttt{\_mm512\_swizzle\_ps}}
\def\storenrngops{\ttt{\_mm512\_storenrngo\_ps}}
\def\castsi{\ttt{\_mm512\_castsi512\_ps}}
\def\permute{\ttt{\_mm512\_permute4f128\_epi32}}
\def\intmask{\ttt{\_mm512\_int2mask}}
\def\loadunpack{\ttt{\_mm512\_mask\_loadunpacklo\_epi32}}
\def\castitops{\ttt{\_mm512\_castsi512\_ps}}
\def\castpstoi{\ttt{\_mm512\_castps\_si512\_ps}}

\begin{figure*}
  %
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      \loadps &  load 16 consecutive floats to register\\
      \fmadps &  multiply/add of 16 floats\\
      \gatherps &  gather 16, possibly disconnected floats from memory\\
      \swizzleps &  reorder a channel, and duplicate across channels\\
      \storenrngops &  store 16 floats to memory without register rewrite or reordering\\
      \castpstoi & reinterpret 16 floats as integers\\
      \castitops & reinterpret 16 integers as floats\\
      \permute &  permute channels; do not change individual channels\\
      \intmask &  \\
      \loadunpack &  \\
      \hline
    \end{tabular}
  \end{center}
  %
  \caption{List of MIC vectorial instructions used.}
\label{fig:MIC-vect-instr}
\end{figure*}
In the vector instructions, \ttt{ps} refers to a float (4 bytes),
while \ttt{epi32} refers to a 4-byte integer. Although we do not
discuss double precision in this paper, \ttt{ds} refers to an 8 byte
double precision real number (i.e., \loadds).  Each vector register is
broken up into four channels of 128 bits each. It is possible to
interchange these channels at a cost of a "few" \qes{exact numbers?}
cycles, and it is also possible to execute swizzle operations within a
channel. For example, if the 128 bytes of each channel are labelled as
$ABCD$, the vector instruction \ttt{\_mm512\_swizzle\_ps(v,
  \_MM\_SWIZ\_REG\_AAAA)} replaces each channel by $AAAA$.

Here is a function that reads in four floats (a,b,c,d) and creates the 16-float vector dddd,cccc,bbbb,aaaa: 
\begin{verbatim}
__m512 read_aaaa(float* a)                                                              
{
    int int_mask_lo = (1 << 0) + (1 << 4) + (1 << 8) + (1 << 12);
    __mmask16 mask_lo = _mm512_int2mask(int_mask_lo);
    __m512 v1_old;
    v1_old = _mm512_setzero_ps();
    v1_old = _mm512_mask_loadunpacklo_ps(v1_old, mask_lo, a);
    v1_old = _mm512_mask_loadunpackhi_ps(v1_old, mask_lo, a);
    v1_old = _mm512_swizzle_ps(v1_old, _MM_SWIZ_REG_AAAA);
    return v1_old;
}
\end{verbatim}

Here is a function that takes four floats from float register \ttt{v1}, and 
places them in each of the four lanes. Because the permulation function only 
operates on 4-byte integers, it is necessary to convert (in place) each float
to an integer (the bit structure is not changed), permute the lanes and cast
the integers back to floats (with no modification of the bit structure)
\begin{verbatim}
__m512 permute(__m512 v1, _MM_PERM_ENUM perm)                                           
{
    __m512i vi = _mm512_castps_si512(v1);
    vi = _mm512_permute4f128_epi32(vi, perm);
    v1 = _mm512_castsi512_ps(vi);
    return v1;
}
\end{verbatim}


\newfig{figures/swizzling.pdf}{\wide}{Duplication of the first four
  bytes of each channel across the entire channel.}{fig:swizzling}

\newfig{figures/channel_permutation.pdf}{\wide}{Channel Permutations
  on the MIC}{fig:permutation}

\newfig{figures/tensor_product.pdf}{\wide}{Tensor scalar product using
  channels and swizzling.}{fig:tensor_product}



\section{Experimental Validation}
\label{sec:expe}

\subsection{Instances}

\todo{How were instances generated. Detail the different type of
  instances and the rational for trying them. Detail how the realistic
  instances were generated. Explain matrix ordering (Kutill McKee).}

\subsection{Bandwidth}


\begin{figure*}

  \subfigure[Bandwidth performance under idealized conditions as a
    function of matrix row size. Entries with "cpp" denote cases where
    coding was performed without MIC vector
    instructions.]{\includegraphics[width=.49\linewidth]{figures/test1_readwrite.pdf}\label{fig:band_rw}}
%
  \subfigure[Bandwidth performance under
  idealized conditions as a function of matrix row size. Entries with
  "cpp" denote cases where coding was performed without MIC vector
  instructions.]{\includegraphics[width=.49\linewidth]{figures/test1_gather.pdf}\label{fig:band_gather}}

  \subfigure[Bandwidth performance under idealized conditions as a
    function of matrix row size. Entries with "cpp" denote cases where
    coding was performed without MIC vector instructions. The greater
    speed of the cpp version is obtained throught the use of
    \ttt{\#Ivdep} {\em and\/} \ttt{\_\_assumed\_aligned}. All memory
    is aligned on 64
    bytes.]{\includegraphics[width=.49\linewidth]{figures/test3_gather.pdf}\label{fig:band_gather_ivdep}}
%
  \subfigure[Bandwidth performance on the host under idealized
    conditions as a function of matrix row size. Entries with "cpp"
    denote cases where coding was performed without AVX vector
    instructions. The speed on the CPU matches the speed with AVX
    instructions. All memory is aligned on 32
    bytes.]{\includegraphics[width=.49\linewidth]{figures/host_test1_readwrite_no_temporal_hint.pdf}\label{fig:read_write}}

\caption{Bandwidth Analysis.\todo{not 100\% sure what this plots are.}
  \todo{can we harmozie y-axis?} \todo{ylabel should read "Bandwidth (GB/s)".}
  \todo{remove title.}}

\end{figure*}

\subsection{Computations}



\begin{figure*}
  \centering

  \subfigure[Performance of $y=Ax$ on the MIC. Squares: base 1/1 case,
    solid circles: 4/4 case implemented in C++, solid triangles: 4/4
    case implemented with MIC vector instructions. $-O3$ compilation
    options. Each group consists of four cases: grids of $64^3$ and
    $96^3$ with and without Reverse Cuthill
    McKee.\todo{I don't think this is supercompact but RBF 3D.}]{\includegraphics[width=.49\linewidth]{figures/mic_performance_nb_threads.pdf}\label{}}
%
  \subfigure[performance on the host on a 64x64x64 matrix. Manual
    vectoization brings significant improvement there as
    well.]{\includegraphics[width=.49\linewidth]{figures/64x64x64.pdf}\label{}}

  \caption{Vectorization matters in practice. Differences in BW
    translate in different performance.\todo{this two plots should ``correlate'' }}
\end{figure*}





\begin{figure*}[t]
  \centering
  \subfigure[Supercompact\todo{is the title? this looks like super compact.}]{\includegraphics[width=.49\linewidth]{figures/supercompact_max_perf.pdf}}
  \subfigure[Compact]{\includegraphics[width=.49\linewidth]{figures/compact_max_perf.pdf}}

  \subfigure[Random]{\includegraphics[width=.49\linewidth]{figures/random_max_perf.pdf}}
  \subfigure[RBF 3D]{\includegraphics[width=.49\linewidth]{figures/rbf_max_perf_run1.pdf}}
  \caption{Performance of "best code" accross instance types. \todo{Why stop at 1.6M
      rows. Is that because of memory limitation? I guess that's what
      it is}\todo{can we remove the title?}\todo{can we harmonize y-axis?}}
\end{figure*}


\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/plot_for_natasha_ncar_end_of_year.pdf}

  \caption{Performance on a MIC of derivative computation for RBFFD
    with 884,736 nodes distributed quasi-randomly in a cube. Single
    derivative of a single function (base case, triangle) and four
    derivatives of four functions (multi case, circles). No bandwidth
    reduction (blue) and Reverse Cuthill-McKee (red).\todo{should we have a similar one for the host?}}
\end{figure}

Derivatives in an RBFFD formulation are expressed as a sparse
matrix/vector multiplication (SpMV). Using the full 61 nodes of a MIC
processor, and all four threads of each node, we achieve lowly 18
Gflops when calculating a single vector of a single
function. Bandwidth reduction has minimal effect since the cache is
large enough to store both the derivative matrix and the solution
vector. We increase the number of computations per byte transferred by
calculating four different derivatives of four different
functions. With bandwidth reduction, we speed the calculation by a
factor of 8, and achieve upwards of 140 Gflops.


\section{Conclusion}
\label{sec:ccl}

In the previous sections, we have explored in the practical
implementation on a MIC architecture of multiple derivative operators
acting on multiple vectors within the context of RBF-RD. Each
derivative has an associated sparse matrix with a fixed number of
non-zeros on each row. While computing a single derivative of multiple
functions is rather common (vectorization occurs over the vectors), we
accelerate the algorithm further by considering multiple derivatives
with corresponding matrices with identical adjacency graphs. We
specialize to four matrices and four derivatives, with 16 outputs,
computed as a sum of outer products.

Using a matrix with $64^3$ (or $96^3$ rows, we achieve a speedup of
xxx relative to the base case of one derivative and one function (1/1
case), relative to a potential speedup of 16. We have achieved a
speed of 100 Gflop/s on 60 cores using 240 threads, or a speed of 61\% of
maximum possible performance. The maximum possible speed for the 1/1
case is around 40 Gflop/s, of which we achieve 20 Gflop/s. On matrices
that exceed the cache, a reverse Cuthill-McKee is applied to reduce the
bandwidth, with an associated speedup of 30\% up to 135 Gflop/s. Our
optimal implementation makes use of the AVX instruction set \todo{that's not AVX on MIC, not sure what the name is}, which
makes use of swizzling and channel swapping operations for an
extremely efficient tensor product implementation. A straightforward
implementation without vectorization runs at around 35 Gflops, or 16\% of peak
performance. RCM has no effect because the number of floating point
operations is excessive (the algorithm does not fill the vector
registers to capacity).

In a future work, we will examine the effect of larger stencil sizes,
double precision, and the actual cost of computing the derivatives
within a fluid simulation using RBF-FD.



\section*{Acknowledgment}
The first and last authors acknowledge NSF funding under NSF grant DMS-\#0934331 (FSU). 


\bibliographystyle{plain} %{IEEEabrv,paper.bib}
\bibliography{paper, bollig}
\end{document}


